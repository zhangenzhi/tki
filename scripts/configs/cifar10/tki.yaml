experiment:
  context:
    name: cifar10 #for each exp
    # devices: 1
    # multi-p: 1
    # dist:
    #   devices: 4
    log_path: ./log

  main_loop:
    warmup: 
      student_nums: 0
      supervisor_iters: 1
    nums: 100
    student_nums: 5

  student:
    name: naive
    dataloader:
      name: cifar10 # cifar100, mnist
      batch_size: 128
      epochs: 100
      da: False # data augumentation
    model:
      name: dnn 
      units: [128,64,32,10] 
      activations: [relu,relu,relu,softmax] 
    loss:
      name: CategoricalCrossentropy
    metrics: 
      name: CategoricalAccuracy
    optimizer:
      name: sgd 
      learning_rate: 0.1 
    train_loop:
        train:
          lr_decay: False # decay
          discount_factor: 0.9
          action:
            style: fix_n
            act_space: [0.1,0.5,1.0,5.0,10.0]
          policy: 
            style: epsilon_greedy
            epsilon: 0.50
        valid:
          q_style: multi_action
          save_knowledge: ["state", "reward"]
          discount_factor: 0.9
          weights_style: flat_imagelization # imagelization
          valid_gap: 100
          online: False
        test:
          epoch: -1
        visual: False

  supervisor:
    name: naive
    dataloader:
      name: dnn_RL
      exp: decay
      replay_window: 10
      batch_size: 32
      epochs: 30
    model:
      name: dnn # default dnn
      units: [128,64,32,5] # default 128,64,32,5
      activations: [relu,relu,relu,linear] # default relu
      downsampling: True
    optimizer:
      name: sgd
      learning_rate: 0.01
    loss:
      name: MeanSquaredError #MeanAbsoluteError 
    metrics: 
      name: MeanSquaredError
    train_loop:
        train:
          lr_decay: False # decay
          weights_style: flat_imagelization # stack_imagelization
        valid:
          valid_gap: 100
          epoch: -1
        test:
          epoch: -1