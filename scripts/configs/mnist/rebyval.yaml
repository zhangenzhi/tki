experiment:
  context:
    name: mnist-mlp-da
    multi-p: False

  main_loop:
    warmup: 
      student_nums: 1
      supervisor_iters: 0
    nums: 0
    student_nums: 0

  student:
    dataloader:
      name: mnist # default cifar10
      batch_size: 128
      epochs: 100
      da: False
    model:
      name: dnn # default dnn
      units: [256,128,64,10] # default 128,64,64,32
      activations: [relu,relu,relu,softmax] # default relu
    loss:
      name: CategoricalCrossentropy
    metrics: 
      name: categorical_accuracy
    optimizer:
      name: sgd # default SGD
      learning_rate: 0.1 # default 0.01
    train_loop:
        train:
          lr_decay: True
        valid:
          weight_space:
            format: sum_reduce
          valid_gap: 10000

  supervisor:
    dataloader:
      name: dnn_sumreduce
      replay_window: 1000
      batch_size: 128
      epochs: 200
    model:
      name: dnn # default dnn
      units: [128,64,32,1] # default 128,64,64,32
      activations: [relu,relu,relu,softplus] # default relu
    optimizer:
      name: sgd
      learning_rate: 0.01
    loss:
      name: MeanAbsoluteError #MeanAbsoluteError 
    train_loop:
        valid:
          valid_gap: 100 